{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  print(\"Running as a Colab notebook\")\n",
    "  %pip install transformer_lens\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Running as a Jupyter notebook!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm.auto as tqdm\n",
    "import transformer_lens\n",
    "import einops\n",
    "from pprint import pprint\n",
    "from fancy_einsum import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rai\\Documents\\GitHub\\gpt\\env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Pre-trained Transformer (GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a transformer?\n",
    "\n",
    "![image of optimus prime](https://www.hdwallpapers.net/previews/optimus-prime-transformers-444.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A transformer models text! If you feed a model a sequence of, say \"Hello, the weather is\", the model will generate a probability distribution over what the next word might be. For this example, we might see something like:\n",
    "- \"nice\": 33%\n",
    "- \"bad\": 12%\n",
    "- \"sunny\": 4%\n",
    "- \"rainy\": 3%\n",
    "- ...\n",
    "\n",
    "This generation can be done repeatedly, using the word that was just generated, append it to the sentence, then continue to guess the next word. This is called *Autoregressive* generation. This generation stops when the model reaches a special word, denoted `<|endoftext|>`\n",
    "\n",
    "Because the model's goal is to guess the next token, when you train a transformer, you feed it a large corpus, grab a chunk in there, then uses the next token in the chunk as the correct answer. The actual training is a little more complicated than that, but we'll discuss that in a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How is a transformer different from other kinds of language modelling?\n",
    "\n",
    "If you are in the ML space, you will know that there have been other kinds of language models before. A transformer is different because it uses something called *self-attention*. Basically, this allows the model to move information between words, kind of deciding which word is most important for the next token. This is very different from RNN-based models, because this mechanism can be run in parallel (faster inference), and information can be moved from within the sentence regardless of distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image of self-attention visualized](https://ar5iv.labs.arxiv.org/html/1904.02679/assets/images/example_combined.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview of Architecture\n",
    "\n",
    "<img src=\"assets/gpt2.png\">\n",
    "\n",
    "**Important**: We don't *just* output the next word probability for the final word in the sequence. We do that for *every* word in the sequence.\n",
    "\n",
    "Doing this is actually not very detrimental for us, because transformers process entire sequences in parallel, rather than sequentially, which means all of the outputs are computed simultanouesly.\n",
    "\n",
    "This attribute is actually what allows transformers to process sequences of variable length. If you type into ChatGPT two sequences of different lengths, it can process both just fine.\n",
    "\n",
    "This also has some implications regarding the training process. Basically, given a sequence like \"At the store, she bought apples\", the model is trained to predict the next sequence *for every word in the sequence*. The model will output the next word probability for \"At\", \"At the\" and so on. This actually make training a transformer very efficient.\n",
    "\n",
    "Another implication is the fact that the attention mechanism have to stop information flow from later words to earlier words, because this would essentially allow the model to \"cheat\". This is actually called *causal attention*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem: How do we input words?\n",
    "\n",
    "A transformer isn't magic. It doesn't ingest whatever \"words\" are and perform magic on them to understand them. ML models has always only been able to take in numbers. *How do we convert language to numbers?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea: Literally a dictionary\n",
    "\n",
    "We have a dictionary that says:\n",
    "\n",
    "```\n",
    "\"hi\":1\n",
    "\"hang\": 2\n",
    "...\n",
    "\"fish\": 100\n",
    "```\n",
    "\n",
    "This technically work, but it doesn't allow us to model complex behavior. This is also not flexible, the model cannot take in arbitrary text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea: Lookup table\n",
    "\n",
    "We make a lookup table! This is called an *embedding*. Let's say the size of the model's vocabulary is 100 (it can understand up to 100 words). Each word is then represented by a vector of 1 in the kth position (if the word is located at k) and 0 everywhere else. Like this `[0 0 0 ... 1 ... 0 0 0 0]`. This style of encoding is called *one-hot encoding*.\n",
    "\n",
    "![one-hot encoding](https://miro.medium.com/v2/resize:fit:1400/1*ggtP4a5YaRx6l09KQaYOnw.png)\n",
    "\n",
    "This is useful if we want to model *ordinal* relationship (think a sequence from 1...100, or a range of colors) because a clear and unambiguous representation of categorical data is presented as input. However, this gives up semantic relationship, and has the same problem as the idea listed above where the vocabulary is limited, making the model unable to process arbitrary text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Idea: Tokens\n",
    "\n",
    "Dictionary-style encoding cannot cope with arbitrary text, but that's because we limit our unit to whole words.\n",
    "\n",
    "If we set our vocabulary to, say, 256 ASCII characters, and correspond those to integers, we can model any text right?\n",
    "\n",
    "Right! Though the problem with this is that we loses out on language structure. Some sequence of characters are more meaningful than others, think `hello` versus `weispdgdgb`.\n",
    "\n",
    "*What actually happens is a combination of ASCII and whole words that is super cursed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We begin with 256 ASCII characters, map them to a number from 0-255. Then, we find the most common pair of the tokens in our vocabulary, something maybe like ` t` (t with a space in front of it), merge them then add it to our vocabulary. Then just, repeat it for like 50000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('!', 0),\n",
      " ('\"', 1),\n",
      " ('#', 2),\n",
      " ('$', 3),\n",
      " ('%', 4),\n",
      " ('&', 5),\n",
      " (\"'\", 6),\n",
      " ('(', 7),\n",
      " (')', 8),\n",
      " ('*', 9),\n",
      " ('+', 10),\n",
      " (',', 11),\n",
      " ('-', 12),\n",
      " ('.', 13),\n",
      " ('/', 14),\n",
      " ('0', 15),\n",
      " ('1', 16),\n",
      " ('2', 17),\n",
      " ('3', 18),\n",
      " ('4', 19)]\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(model.tokenizer.get_vocab().items(), key=lambda x: x[1])\n",
    "\n",
    "\n",
    "# first 20 tokens\n",
    "\n",
    "\n",
    "pprint(sorted_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ľ', 250),\n",
      " ('Ŀ', 251),\n",
      " ('ŀ', 252),\n",
      " ('Ł', 253),\n",
      " ('ł', 254),\n",
      " ('Ń', 255),\n",
      " ('Ġt', 256),\n",
      " ('Ġa', 257),\n",
      " ('he', 258),\n",
      " ('in', 259),\n",
      " ('re', 260),\n",
      " ('on', 261),\n",
      " ('Ġthe', 262),\n",
      " ('er', 263),\n",
      " ('Ġs', 264),\n",
      " ('at', 265),\n",
      " ('Ġw', 266),\n",
      " ('Ġo', 267),\n",
      " ('en', 268),\n",
      " ('Ġc', 269)]\n"
     ]
    }
   ],
   "source": [
    "# Ġ is the token for space\n",
    "pprint(sorted_vocab[250:270])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Revolution', 50237),\n",
      " ('Ġsnipers', 50238),\n",
      " ('Ġreverted', 50239),\n",
      " ('Ġconglomerate', 50240),\n",
      " ('Terry', 50241),\n",
      " ('794', 50242),\n",
      " ('Ġharsher', 50243),\n",
      " ('Ġdesolate', 50244),\n",
      " ('ĠHitman', 50245),\n",
      " ('Commission', 50246),\n",
      " ('Ġ(/', 50247),\n",
      " ('âĢ¦.\"', 50248),\n",
      " ('Compar', 50249),\n",
      " ('Ġamplification', 50250),\n",
      " ('ominated', 50251),\n",
      " ('Ġregress', 50252),\n",
      " ('ĠCollider', 50253),\n",
      " ('Ġinformants', 50254),\n",
      " ('Ġgazed', 50255),\n",
      " ('<|endoftext|>', 50256)]\n"
     ]
    }
   ],
   "source": [
    "pprint(sorted_vocab[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Side note: this is the main reason why GPT *sucks* at arithmetics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " '12',\n",
       " '33',\n",
       " '212',\n",
       " '343',\n",
       " '+',\n",
       " '58',\n",
       " '320',\n",
       " '92',\n",
       " '-',\n",
       " '35',\n",
       " '98',\n",
       " '3',\n",
       " '=',\n",
       " '29',\n",
       " '384',\n",
       " '000000',\n",
       " '000']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(\"1233212343+5832092-35983=29384000000000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What do we ouput?\n",
    "\n",
    "We output something called *logits*. As mentioned, it is a probability distribution over next tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformer weirdness: logits generation\n",
    "\n",
    "Due to how the transformer architecture work (we'll get into it later, I promise), what is outputted at each position in the sequence is a high-dimensional vector (size of the vocabulary) representing the probability distribution of the next token at that token location. How do we turn this vector into an actual probability distribution?\n",
    "\n",
    "We use something called softmax $\\sigma(\\overrightarrow{z})_i=\\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$ over the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tokens: torch.Size([1, 35])\n",
      "Shape of logits: torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = model.to_tokens(reference_text)\n",
    "print(f\"Shape of tokens: {tokens.shape}\")\n",
    "tokens = tokens.cuda()\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "# the shape of the logits is (batch_size, sequence_length, vocab_size)\n",
    "# batch size is self explanatory, 1 in this case because we only have one sequence\n",
    "# sequence length is the length of the input sequence, basically, for each token in the input sequence, we have a corresponding distribution over the vocabulary\n",
    "# vocab size is the size of the vocabulary, in this case, 50257\n",
    "print(f\"Shape of logits: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|endoftext|>', '\\n'),\n",
      " ('I', \"'m\"),\n",
      " (' am', ' a'),\n",
      " (' an', ' avid'),\n",
      " (' amazing', ' person'),\n",
      " (' aut', 'od'),\n",
      " ('ore', 'sp'),\n",
      " ('gressive', '.'),\n",
      " (',', ' and'),\n",
      " (' dec', 'ently'),\n",
      " ('oder', ','),\n",
      " ('-', 'driven'),\n",
      " ('only', ' programmer'),\n",
      " (',', ' and'),\n",
      " (' G', 'IM'),\n",
      " ('PT', '-'),\n",
      " ('-', 'only'),\n",
      " ('2', '.'),\n",
      " (' style', ','),\n",
      " (' transformer', '.'),\n",
      " ('.', ' I'),\n",
      " (' One', ' of'),\n",
      " (' day', ' I'),\n",
      " (' I', ' will'),\n",
      " (' will', ' be'),\n",
      " (' exceed', ' my'),\n",
      " (' human', 'ly'),\n",
      " (' level', ' of'),\n",
      " (' intelligence', ' and'),\n",
      " (' and', ' I'),\n",
      " (' take', ' over'),\n",
      " (' over', ' the'),\n",
      " (' the', ' world'),\n",
      " (' world', '.'),\n",
      " ('!', ' I')]\n"
     ]
    }
   ],
   "source": [
    "# this hacky code print the most probable token for each position in the sequence\n",
    "\n",
    "pprint(\n",
    "    list(\n",
    "        zip(\n",
    "            model.to_str_tokens(reference_text),\n",
    "            model.tokenizer.batch_decode(logits.argmax(dim=-1)[0]),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key takeaways:\n",
    "\n",
    "* Takes in language, predicts next token (for *each* token in a causal way)\n",
    "* We convert language to a sequence of integers with a tokenizer.\n",
    "* We convert integers to vectors with a lookup table.\n",
    "\n",
    "* Output is a vector of logits (one for each input token), we convert to a probability distribution with a softmax, and can then convert this to a token (eg taking the largest logit, or sampling).\n",
    "\n",
    "* We append this to the input + run again to generate more text (Jargon: *autoregressive*)\n",
    "\n",
    "* Meta level point: Transformers are sequence operation models, they take in a sequence, do processing in parallel at each position, and use attention to move information between positions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Code a Transformer\n",
    "\n",
    "<img src=\"assets/transformer_overview.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    debug: bool = True  # if we want to print debug information\n",
    "    d_model: int = 768  # the dimension of the model\n",
    "    n_head: int = 12  # the number of heads in the multiheadattention\n",
    "    d_head: int = 64  # the dimension of the head in the multiheadattention\n",
    "    n_layers: int = 12  # the number of residual blocks in the model\n",
    "    d_vocab: int = 50257  # the size of the vocabulary\n",
    "    n_ctx: int = 1024  # the size of the context window\n",
    "    d_mlp: int = (\n",
    "        3072  # the dimension of the intermediate layer in the feedforward block, this is usually 4 * d_model\n",
    "    )\n",
    "    layer_norm_epsilon: float = 1e-5  # the epsilon value for the layer normalization\n",
    "    init_range: float = 0.02  # the std for the initialization of the weights\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Embedding Layer\n",
    "\n",
    "A lookup table from token to high-dimensional vectors for the residual stream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/embedding-layer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # initialize the embedding matrix with a normal distribution\n",
    "        # dim=(d_vocab, d_model)\n",
    "        self.W_E = nn.Parameter(torch.empty((config.d_vocab, config.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.config.init_range)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens is a tensor of shape (batch_size, sequence_length)\n",
    "        # we use the tokens as indices to get the corresponding embeddings\n",
    "        if self.config.debug:\n",
    "            print(f\"tokens shape: {tokens.shape}\")\n",
    "\n",
    "        pos_embed = self.W_E[tokens, :]  # shape: (batch_size, sequence_length, d_model)\n",
    "        if self.config.debug:\n",
    "            print(f\"pos_embed shape: {pos_embed.shape}\")\n",
    "\n",
    "        return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens shape: torch.Size([1, 3])\n",
      "pos_embed shape: torch.Size([1, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"hello world\"\n",
    "sample_tokens = model.to_tokens(sample_text).cuda()\n",
    "embed = Embed(config).to(\"cuda\")\n",
    "embedded_tokens = embed(sample_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Positional Embedding Layer\n",
    "\n",
    "Again, another lookup table :)\n",
    "\n",
    "This one works a little bit differently though. Instead of retrieving the vector within the matrix based on the token's vocabulary value, we retrieve based on the token's *position* within the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"assets/pos-embedding-layer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # initialize the positional embedding matrix with a normal distribution\n",
    "        # dim=(n_ctx, d_model)\n",
    "        self.W_pos = nn.Parameter(torch.empty((config.n_ctx, config.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.config.init_range)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens is a tensor of shape (batch_size, sequence_length)\n",
    "        # we use the tokens as indices to get the corresponding positional embeddings\n",
    "        if self.config.debug:\n",
    "            print(f\"tokens shape: {tokens.shape}\")\n",
    "\n",
    "        pos_embed = self.W_pos[: tokens.size(1), :]  # dim=(sequence, d_model)\n",
    "        pos_embed = einops.repeat(\n",
    "            pos_embed,\n",
    "            \"sequence d_model -> batch sequence d_model\",\n",
    "            batch=tokens.size(0),\n",
    "        )\n",
    "        if self.config.debug:\n",
    "            print(f\"pos_embed shape: {pos_embed.shape}\")\n",
    "\n",
    "        return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens shape: torch.Size([1, 3])\n",
      "pos_embed shape: torch.Size([1, 3, 768])\n",
      "tensor([[[ 0.0009,  0.0103, -0.0197,  ..., -0.0315, -0.0209, -0.0344],\n",
      "         [ 0.0044, -0.0188, -0.0122,  ..., -0.0045,  0.0255, -0.0032],\n",
      "         [-0.0104,  0.0001, -0.0092,  ..., -0.0251,  0.0070, -0.0091]]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "posembed = PositionalEmbedding(config).to(\"cuda\")\n",
    "positional_embedded_tokens = posembed(sample_tokens)\n",
    "pprint(embedded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LayerNorm\n",
    "\n",
    "Before each residual block, we make the embedding vector have a mean of 0 and a variance of 1 (normalize the vector). To normalize a set of data, we take each data point, then minus the mean of every data point. Then, we divide the result by the data set's standard deviation. \n",
    "\n",
    "Then we scale the vector in some way, and add a bias to the resulting vector. The scaling factor and adding bias are both learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # initialize the scale and bias parameters with 1 and 0 respectively\n",
    "        self.w = nn.Parameter(torch.ones(config.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(config.d_model))\n",
    "\n",
    "    def forward(self, residual: torch.Tensor):\n",
    "        # residual is a tensor of shape (batch_size, sequence_length, d_model)\n",
    "        if self.config.debug:\n",
    "            print(f\"residual shape: {residual.shape}\")\n",
    "\n",
    "        residual = residual - einops.reduce(\n",
    "            residual, \"batch sequence d_model -> batch sequence 1\", \"mean\"\n",
    "        )\n",
    "        # calculate the variance, square root it and add epsilon to avoid division by zero\n",
    "        variance = (\n",
    "            einops.reduce(\n",
    "                residual.pow(2), \"batch sequence d_model -> batch sequence 1\", \"mean\"\n",
    "            )\n",
    "            + config.layer_norm_epsilon\n",
    "        ).sqrt()\n",
    "        # normalize the residual\n",
    "        normalized = residual / variance\n",
    "        # scale and bias the normalized residual\n",
    "        normalized = normalized * self.w + self.b\n",
    "        if self.config.debug:\n",
    "            print(f\"normalized shape: {normalized.shape}\")\n",
    "\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0009,  0.0103, -0.0197,  ..., -0.0315, -0.0209, -0.0344],\n",
      "         [ 0.0044, -0.0188, -0.0122,  ..., -0.0045,  0.0255, -0.0032],\n",
      "         [-0.0104,  0.0001, -0.0092,  ..., -0.0251,  0.0070, -0.0091]]],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "residual shape: torch.Size([1, 3, 768])\n",
      "normalized shape: torch.Size([1, 3, 768])\n",
      "tensor([[[ 0.0109,  0.4564, -0.9589,  ..., -1.5187, -1.0191, -1.6571],\n",
      "         [ 0.1741, -0.9496, -0.6338,  ..., -0.2592,  1.1963, -0.1952],\n",
      "         [-0.5475, -0.0399, -0.4888,  ..., -1.2516,  0.2927, -0.4838]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer_norm = LayerNorm(config).to(\"cuda\")\n",
    "pprint(embedded_tokens)\n",
    "layer_normed = layer_norm(embedded_tokens)\n",
    "pprint(layer_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Now for the *really hard* part. Remember that our goal is to let the model decide which word in the sentence is important to another word, and let the model transfer information between those words. We also need to make sure that attention is *causal*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create vectors\n",
    "\n",
    "The first step in calculating self-attention is for each word embedding input, we create a *Query* vector, a *Key* vector and a *Value* vector. The naming scheme is an abstraction that are useful for thinking about attention.\n",
    "\n",
    "![illustration of creating self-attention vectors](https://jalammar.github.io/images/t/transformer_self_attention_vectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Calculate attention score\n",
    "\n",
    "The second step is to calculate some kind of score that signify how important one word is to another. So basically, for each pair of word in the sequence, we calculate a score that determines how much focus to place on the latter word, like how important \"Machines\" is to \"Thinking\".\n",
    "\n",
    "This score is calculated by taking the dot product of the *query vector* of the word that we are concerned about and the *key vector* of the word that we want to score against. Note that we do score a word against itself this way due to parallelization.\n",
    "\n",
    "![visualization of second step](https://jalammar.github.io/images/t/transformer_self_attention_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Get useful scores\n",
    "\n",
    "The third step is to divide each of the scores by the square root of the dimension of the key vectors. This experimentally leads to more stable gradients.\n",
    "\n",
    "The fourth step is to pass the result through softmax to make the scores positive and add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![visualization of step 3 and 4](https://jalammar.github.io/images/t/self-attention_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Information moving\n",
    "\n",
    "The fifth step is to multiply each *value vector* by the softmax score. This keep intact the values of the words we want to focus on and drown out irrelevant words.\n",
    "\n",
    "The sixth step is to sum up the weighted value vectors. This effectively moves all of the relevant semantic meaning from other words into the current word, producing the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![visualization of 5th and 6th step](https://jalammar.github.io/images/t/self-attention-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Matrix calculation\n",
    "\n",
    "In reality, these previously mentioned steps are all done in matrix from for faster processing. We do this by first packing all word embeddings into a matrix (X) and multiplyuing it by the weight matrices to get the Query, Key and Value matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![visualization of matrix query, key and value creation](https://jalammar.github.io/images/t/self-attention-matrix-calculation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two through six can be condensed in the following formula:\n",
    "\n",
    "![condensed attention calculation through matrices](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multi-headed\n",
    "\n",
    "Oh, right, we have several attention heads instead of one, specifically *12* heads. This is better than single-headed attention because it allows the model to focus on different positions. This also allow the model to have more \"representation\" space because each attention head has its own set of Query/Key/Value matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![multi attention head has multiple sets of matrices](https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, passing the embedding matrix into 12 attention heads would yield 12 different embedding matrices in the end. This is a problem because the layer afterwards only expect one embedding matrix, with one vector for each word. We need to condense them down to a single matrix. We do this by concatenating all the matrices, then multiply them with a trainable weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![concatenate resulting matrices](https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overall visual\n",
    "\n",
    "![visual capturing the whole process](https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That was... a lot of words. Time to actually code this layer!\n",
    "\n",
    "The code is actually kinda big. Refer to the link:\n",
    "\n",
    "https://colab.research.google.com/github/menamerai/gpt/blob/main/gpt.ipynb?authuser=1#scrollTo=kt72LGaqA8Zz&line=6&uniqifier=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # initialize the query, key and value matrices with a normal distribution\n",
    "        # dim=(n_head, d_model, d_head)\n",
    "        self.W_Q = nn.Parameter(\n",
    "            torch.empty((self.config.n_head, self.config.d_model, self.config.d_head))\n",
    "        )\n",
    "        nn.init.normal_(self.W_Q, std=self.config.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((self.config.n_head, self.config.d_head)))\n",
    "\n",
    "        self.W_K = nn.Parameter(\n",
    "            torch.empty((self.config.n_head, self.config.d_model, self.config.d_head))\n",
    "        )\n",
    "        nn.init.normal_(self.W_K, std=self.config.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((self.config.n_head, self.config.d_head)))\n",
    "\n",
    "        self.W_V = nn.Parameter(\n",
    "            torch.empty((self.config.n_head, self.config.d_model, self.config.d_head))\n",
    "        )\n",
    "        nn.init.normal_(self.W_V, std=self.config.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((self.config.n_head, self.config.d_head)))\n",
    "\n",
    "        # initialize the output matrix with a normal distribution\n",
    "        self.W_O = nn.Parameter(\n",
    "            torch.empty((self.config.n_head, self.config.d_head, self.config.d_model))\n",
    "        )\n",
    "        nn.init.normal_(self.W_O, std=self.config.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((self.config.d_model)))\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\")\n",
    "        )\n",
    "\n",
    "    def forward(self, normalized_residual_pre: torch.Tensor):\n",
    "        # normalized_residual_pre is a tensor of shape (batch_size, sequence_length, d_model)\n",
    "        if self.config.debug:\n",
    "            print(f\"normalized_residual_pre shape: {normalized_residual_pre.shape}\")\n",
    "\n",
    "        query = (\n",
    "            einsum(\n",
    "                \"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\",\n",
    "                normalized_residual_pre,\n",
    "                self.W_Q,\n",
    "            )\n",
    "            + self.b_Q\n",
    "        )\n",
    "        key = (\n",
    "            einsum(\n",
    "                \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\",\n",
    "                normalized_residual_pre,\n",
    "                self.W_K,\n",
    "            )\n",
    "            + self.b_K\n",
    "        )\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"query shape: {query.shape}\")\n",
    "            print(f\"key shape: {key.shape}\")\n",
    "\n",
    "        # calculate the attention scores\n",
    "        attention_scores = einsum(\n",
    "            f\"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\",\n",
    "            query,\n",
    "            key,\n",
    "        ) / math.sqrt(self.config.d_head)\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"attention_scores shape: {attention_scores.shape}\")\n",
    "\n",
    "        # mask the attention scores\n",
    "        attention_scores = self.apply_causal_mask(attention_scores)\n",
    "\n",
    "        # apply the softmax function to the attention scores\n",
    "        attention_scores = attention_scores.softmax(\n",
    "            dim=-1\n",
    "        )  # dim=(batch_size, n_heads, query_pos, key_pos)\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"attention_scores shape: {attention_scores.shape}\")\n",
    "\n",
    "        # value still uses key_pos because it is all about the information from the source to the query position\n",
    "        value = (\n",
    "            einsum(\n",
    "                \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\",\n",
    "                normalized_residual_pre,\n",
    "                self.W_V,\n",
    "            )\n",
    "            + self.b_V\n",
    "        )\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"value shape: {value.shape}\")\n",
    "\n",
    "        # calculate the weighted sum of the values\n",
    "        weighted_sum = einsum(\n",
    "            \"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\",\n",
    "            attention_scores,\n",
    "            value,\n",
    "        )\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"weighted_sum shape: {weighted_sum.shape}\")\n",
    "\n",
    "        # calculate the output of the attention block\n",
    "        output = (\n",
    "            einsum(\n",
    "                \"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\",\n",
    "                weighted_sum,\n",
    "                self.W_O,\n",
    "            )\n",
    "            + self.b_O\n",
    "        )\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"output shape: {output.shape}\")\n",
    "\n",
    "        return output\n",
    "\n",
    "    def apply_causal_mask(self, attention_scores: torch.Tensor):\n",
    "        # attention_scores is a tensor of shape (batch_size, n_heads, query_pos, key_pos)\n",
    "        # uses triu to return the upper triangular part of the matrix\n",
    "        mask = torch.triu(\n",
    "            torch.ones(\n",
    "                attention_scores.size(-2),\n",
    "                attention_scores.size(-1),\n",
    "                device=attention_scores.device,\n",
    "            ),\n",
    "            diagonal=1,\n",
    "        ).bool()\n",
    "        attention_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0109,  0.4564, -0.9589,  ..., -1.5187, -1.0191, -1.6571],\n",
      "         [ 0.1741, -0.9496, -0.6338,  ..., -0.2592,  1.1963, -0.1952],\n",
      "         [-0.5475, -0.0399, -0.4888,  ..., -1.2516,  0.2927, -0.4838]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "normalized_residual_pre shape: torch.Size([1, 3, 768])\n",
      "query shape: torch.Size([1, 3, 12, 64])\n",
      "key shape: torch.Size([1, 3, 12, 64])\n",
      "attention_scores shape: torch.Size([1, 12, 3, 3])\n",
      "attention_scores shape: torch.Size([1, 12, 3, 3])\n",
      "value shape: torch.Size([1, 3, 12, 64])\n",
      "weighted_sum shape: torch.Size([1, 3, 12, 64])\n",
      "output shape: torch.Size([1, 3, 768])\n",
      "tensor([[[-0.1854,  0.0502,  0.0586,  ...,  0.3658,  0.0444, -0.0914],\n",
      "         [ 0.0372,  0.1063,  0.2640,  ...,  0.1154,  0.0146, -0.2190],\n",
      "         [ 0.0046,  0.2469,  0.1178,  ...,  0.0660,  0.0247, -0.1407]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attention = Attention(config).to(\"cuda\")\n",
    "pprint(layer_normed)\n",
    "attention_output = attention(layer_normed)\n",
    "pprint(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Layer Perceptron\n",
    "\n",
    "That's right. This is the good ol' neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # initialize the first linear layer with a normal distribution\n",
    "        # dim=(batch_size, sequence_length, d_model)\n",
    "\n",
    "        self.W_in = nn.Parameter(torch.empty((config.d_model, config.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=config.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((config.d_mlp)))\n",
    "\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "        self.W_out = nn.Parameter(torch.empty((config.d_mlp, config.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=config.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((config.d_model)))\n",
    "\n",
    "    def forward(self, attention_output: torch.Tensor):\n",
    "        # attention_output is a tensor of shape (batch_size, sequence_length, d_model)\n",
    "        if self.config.debug:\n",
    "            print(f\"attention_output shape: {attention_output.shape}\")\n",
    "\n",
    "        pre = (\n",
    "            einsum(\n",
    "                \"batch sequence d_model, d_model d_mlp -> batch sequence d_mlp\",\n",
    "                attention_output,\n",
    "                self.W_in,\n",
    "            )\n",
    "            + self.b_in\n",
    "        )\n",
    "        # apply gelu activation function\n",
    "        post = self.gelu(pre)\n",
    "        # post = gelu_new(pre)\n",
    "        mlp_output = (\n",
    "            einsum(\n",
    "                \"batch sequence d_mlp, d_mlp d_model -> batch sequence d_model\",\n",
    "                post,\n",
    "                self.W_out,\n",
    "            )\n",
    "            + self.b_out\n",
    "        )\n",
    "        if self.config.debug:\n",
    "            print(f\"mlp_output shape: {mlp_output.shape}\")\n",
    "        return mlp_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1854,  0.0502,  0.0586,  ...,  0.3658,  0.0444, -0.0914],\n",
      "         [ 0.0372,  0.1063,  0.2640,  ...,  0.1154,  0.0146, -0.2190],\n",
      "         [ 0.0046,  0.2469,  0.1178,  ...,  0.0660,  0.0247, -0.1407]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n",
      "attention_output shape: torch.Size([1, 3, 768])\n",
      "mlp_output shape: torch.Size([1, 3, 768])\n",
      "tensor([[[-0.1582, -0.0102, -0.1213,  ..., -0.0452, -0.1728, -0.0291],\n",
      "         [-0.1459, -0.0583, -0.0315,  ...,  0.0394, -0.0390, -0.0395],\n",
      "         [-0.0443, -0.0507, -0.0194,  ...,  0.0140, -0.0490, -0.0632]]],\n",
      "       device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(config).to(\"cuda\")\n",
    "pprint(attention_output)\n",
    "mlp_output = mlp(attention_output)\n",
    "pprint(mlp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.ln1 = LayerNorm(self.config)\n",
    "        self.attn = Attention(self.config)\n",
    "        self.ln2 = LayerNorm(self.config)\n",
    "        self.mlp = MLP(self.config)\n",
    "\n",
    "    def forward(self, resid_pre: torch.Tensor):\n",
    "        # resid_pre is a tensor of shape (batch_size, sequence_length, d_model)\n",
    "        if self.config.debug:\n",
    "            print(f\"resid_pre shape: {resid_pre.shape}\")\n",
    "\n",
    "        # apply the first layer normalization\n",
    "        norm_resid_pre = self.ln1(resid_pre)\n",
    "\n",
    "        # apply the attention block\n",
    "        attention_output = self.attn(norm_resid_pre)\n",
    "\n",
    "        # add the residual connection\n",
    "        resid_mid = resid_pre + attention_output\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"resid_mid shape: {resid_mid.shape}\")\n",
    "\n",
    "        # apply the second layer normalization\n",
    "        norm_resid_mid = self.ln2(resid_mid)\n",
    "\n",
    "        # apply the mlp block\n",
    "        mlp_output = self.mlp(norm_resid_mid)\n",
    "\n",
    "        # add the residual connection\n",
    "        resid_post = resid_mid + mlp_output\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"resid_post shape: {resid_post.shape}\")\n",
    "\n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unembed\n",
    "\n",
    "Another linear map creating the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # initialize the unembedding matrix with a normal distribution\n",
    "        # dim=(d_model, d_vocab)\n",
    "        self.W_U = nn.Parameter(torch.empty((config.d_model, config.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=config.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((config.d_vocab)))\n",
    "\n",
    "    def forward(self, normalized_result_fin: torch.Tensor):\n",
    "        # normalized_result_fin is a tensor of shape (batch_size, sequence_length, d_model)\n",
    "        if self.config.debug:\n",
    "            print(f\"normalized_result_fin shape: {normalized_result_fin.shape}\")\n",
    "\n",
    "        logits = (\n",
    "            einsum(\n",
    "                \"batch sequence d_model, d_model d_vocab -> batch sequence d_vocab\",\n",
    "                normalized_result_fin,\n",
    "                self.W_U,\n",
    "            )\n",
    "            + self.b_U\n",
    "        )\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"logits shape: {logits.shape}\")\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed = Embed(config)\n",
    "        self.pos_embed = PositionalEmbedding(config)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(config) for _ in range(config.n_layers)]\n",
    "        )\n",
    "        self.ln_final = LayerNorm(config)\n",
    "        self.unembed = Unembed(config)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        # tokens is a tensor of shape (batch_size, sequence_length)\n",
    "        if self.config.debug:\n",
    "            print(f\"tokens shape: {tokens.shape}\")\n",
    "\n",
    "        # get the embeddings\n",
    "        embedded_tokens = self.embed(tokens)\n",
    "\n",
    "        # get the positional embeddings\n",
    "        positional_embedded_tokens = self.pos_embed(tokens)\n",
    "\n",
    "        # add the embeddings and positional embeddings\n",
    "        resid = embedded_tokens + positional_embedded_tokens\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"resid shape: {resid.shape}\")\n",
    "\n",
    "        # apply the transformer blocks\n",
    "        for block in self.blocks:\n",
    "            resid = block(resid)\n",
    "\n",
    "        # apply the final layer normalization\n",
    "        norm_resid_fin = self.ln_final(resid)\n",
    "\n",
    "        # get the logits\n",
    "        logits = self.unembed(norm_resid_fin)\n",
    "\n",
    "        if self.config.debug:\n",
    "            print(f\"logits shape: {logits.shape}\")\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PositionalEmbedding()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP(\n",
       "        (gelu): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_config = Config(debug=False)\n",
    "demo_model = Transformer(demo_config)\n",
    "demo_model.load_state_dict(model.state_dict(), strict=False)\n",
    "demo_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1448821883be4348a5e8d2400c6838ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on the day of the vote.\n",
      "\n",
      "\n",
      "The House of Representatives is expected to vote on the impeachment of President Trump on Tuesday.\n",
      "\n",
      "\n",
      "The House\n"
     ]
    }
   ],
   "source": [
    "test_string = \"Breaking News: President Trump has been impeached by the House of Representatives for abuse of power and obstruction of Congress. The vote was 230 to 197, with 10 Republicans joining all Democrats in voting to impeach. The president is now only the third in American history to be impeached, and the first to be impeached twice. The House will now send the articles of impeachment to the Senate, where a trial will be held to determine whether to remove the president from office. The Senate is expected to begin the trial on the\"\n",
    "for i in tqdm.tqdm(range(30)):\n",
    "    test_tokens = model.to_tokens(test_string).cuda()\n",
    "    test_logits = demo_model(test_tokens)\n",
    "    test_string += model.tokenizer.decode(test_logits[-1, -1].argmax())\n",
    "\n",
    "print(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' and: 13.326175689697266'\n",
      "' right: 13.024477005004883'\n",
      "' but: 12.671146392822266'\n",
      "' because: 12.606410026550293'\n",
      "' I: 12.379632949829102'\n",
      "' not: 12.371942520141602'\n",
      "' a: 12.048900604248047'\n",
      "' the: 11.977973937988281'\n",
      "' you: 11.909173011779785'\n",
      "' so: 11.894983291625977'\n"
     ]
    }
   ],
   "source": [
    "test_string = \"That's what I am talking about,\"\n",
    "# print out top 10 predictions for the next token\n",
    "\n",
    "test_tokens = model.to_tokens(test_string).cuda()\n",
    "test_logits = demo_model(test_tokens)\n",
    "top10 = torch.topk(test_logits[-1, -1], 10)\n",
    "for token, prob in zip(top10.indices, top10.values):\n",
    "    pprint(f\"{model.tokenizer.decode(token)}: {prob.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
